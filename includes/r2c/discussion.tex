\section{Discussion}\label{sec:r2c:discussion}
%\begin{figure}[t!]
%  \centering
%  \input{stats/20220519-boxplot-variance-diff-horizontal.pgf}
%  \caption{Performance impact variance of differently diversified variants on selected benchmarks on our \textsf{TR 3970X} A system.}
%  \label{r2c:fig:intra-variant-difference}
%\end{figure}

\subsection{Performance}\label{r2c:ss:discussion-evaluation}

\fbetodo{Add section about profile-guided optimization}
As evidenced by the benchmarks, the optimized \gls{BTRA} setup sequence improves \rtwoc{}'s performance considerably (see \cref{r2c:fig:perf-components}).
While our implementation uses AVX2 vector instructions, falling back to SSE vector instructions would be an alternative for more feature constrained CPUs.
For CPUs without vector extensions the \code{push} based setup sequence provides a viable alternative without loss of security.
The difference between the \code{push} and AVX2 setup sequence (see \cref{r2c:fig:perf-components}) suggests that instruction cache pressure may contribute to the overhead, although our counter analysis does not isolate AVX2 vs \code{push}.
Similarly, prolog trap insertion likely adds instruction-cache pressure by expanding hot code.

% We speculate that at the other end of the spectrum an implementation with AVX512 registers could improve performance even more.
% We could not evaluate this effect due to the lack of compatible hardware.


\begin{table}[t]
    \centering
    \sisetup{
        table-alignment-mode=none,
        table-number-alignment=right,
    }
    \begin{tabular}{lS[table-format=12.0]}
        \toprule
        Benchmark              & {Call Frequency} \\
        \midrule
        \propername{perlbench} & 9435182963       \\
        \propername{gcc}       & 7471474392       \\
        \propername{mcf}       & 38657893688      \\
        \propername{lbm}       & 20906700         \\
        \propername{omnetpp}   & 23536583520      \\
        \propername{xalancbmk} & 12430137048      \\
        \propername{x264}      & 3400115007       \\
        \propername{deepsjeng} & 11366032234      \\
        \propername{imagick}   & 10441212712      \\
        \propername{leela}     & 13108456661      \\
        \propername{nab}       & 135237228510     \\
        \propername{xz}        & 3287645643       \\
        \bottomrule
    \end{tabular}
    \caption{Median call frequencies of SPEC CPU 2017 benchmarks across all inputs.}
    \label{tab:call-frequencies}
\end{table}

\Cref{r2c:fig:perf-full} shows that benchmarks with a large number of functions and function calls are affected most by \rtwoc{}.
\rtwoc{} adds \glspl{BTRA} \emph{per call site}, explaining the overhead for function heavy benchmarks.
To test this correlation with call frequency, we instrumented the SPEC CPU benchmark programs to count the number of executed call instructions.
Our instrumentation ignores tail calls because tail calls do not push a return address to the stack and, thus, no \glspl{BTRA} are inserted.
\Cref{tab:call-frequencies} shows the median number of calls performed by the SPEC CPU 2017 benchmarks.
For each benchmark we took the median call frequencies across all inputs.
The data suggests that there is only a weak direct correlation between calls and the overhead:
\propername{Perlbench}, for example, has less than half the number of calls as \propername{omnetpp}, but shows a similar overhead.
The mere call count, therefore, does not sufficiently explain the performance profile.


\begin{table}[t]
    \centering
    \caption{Performance overhead (\%) by \gls{BTRA} count relative to baseline.
    All other \rtwoc components use the same configuration as full \rtwoc (see \cref{r2c:sss:eval-btra}).}
    \label{r2c:tab:btra-steps-overhead}
    \footnotesize
    \begin{tabular}{l *{6}{r}}
        \toprule
        Benchmark & 2 \glspl{BTRA} & 6 \glspl{BTRA} & 10 \glspl{BTRA} & 14 \glspl{BTRA} & 18 \glspl{BTRA} & 22 \glspl{BTRA} \\
        \midrule
        \fileInput{generated/r2c/btra-steps}
        \bottomrule
    \end{tabular}
\end{table}

A possible explanation for the super-additive overhead (sum of component-overheads is less than actual total overhead) in \propername{mcf} and \propername{omnetpp} are memory stalls.
Both \propername{mcf} and \propername{omnetpp} are memory-bound benchmarks and memory stalls can hide a certain number of additional instructions.
Thus, the stalls might hide the instructions of each component in isolation, but the stalls might be too few to hide all components combined.

\subsubsection{Detailed Performance Counter Analysis}\label{r2c:sss:perf-counters}
The example of call count and performance overhead differences between \propername{perlbench} and \propername{omnetpp} above showed that instructions and call counts alone give an incomplete picture.
To better understand the exact causes of the overhead, we recorded \code{perf} counters for benchmarks with overhead on \propername{i9}.
Note that we did not record the counters with the same seeds that were used for the initial performance evaluation.
The difference in seeds and the sampling inaccuracies introduced by \code{perf} can cause slight differences in the observed cycle overhead compared to the runtime overhead described in \cref{r2c:fig:perf-full}.
The relative differences between configurations and the baseline as well as between counters stay intact, however.
We choose \propername{i9} instead of \propername{EPYC} for the \code{perf} analysis because of the richer set of performance counters on the Intel machine.


\begin{figure}[t]
    \centering
    \begingroup
    \pgfplotsset{
        every axis/.append style={
            yticklabel style={font=\footnotesize},
            xticklabel style={font=\footnotesize},
            execute at begin axis={
                \pgfplotsset{
                    width=\textwidth,
                    height=0.8\linewidth}
            },
        },
    }
    \resizebox{\textwidth}{!}{\input{figures/r2c/btra-instr-vs-cycles}}%
    \endgroup
    \caption{The relationship between instruction overhead and cycle overhead.
    The leftmost points resemble a configuration with 2 \glspl{BTRA}, whereas the rightmost points are 22 \glspl{BTRA} (see \cref{r2c:sss:perf-counters} for details).}
    \label{r2c:fig:btra-instr-vs-cycles}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/r2c/stall-hiding}%
    \caption{Conceptual figure of stall hiding.
    Instructions can execute \enquote{for free} during times when the CPU has to wait for the result of a stalling instruction.
    Which instructions can execute during a stall depends, among other things, on the inter-instruction dependencies.}
    \label{r2c:fig:stall-hiding}
\end{figure}

Three performance counters are particularly important when interpreting the results: cycles, instructions, and Intel's \gls{TMA} counters\cite{yasin2014}.
The cycles counter tells us the total number of cycles the benchmark run took.
Given a fixed \gls{CPU} frequency, cycles directly translate to execution time.
Instructions, on the other hand, tell us how many instructions the \gls{CPU} executed.
A single instruction can (and usually does) take more than one cycle.
At the same time, superscalar, pipelined \glspl{CPU} (all \glspl{CPU} in our evaluation) overlap instruction execution and can execute multiple micro-operations in parallel, depending on the instruction type.
As a result, the ratio of instructions and cycles, called \gls{IPC}, can be both smaller than, equal to, or greater than 1.
Intel's \gls{TMA} counters are subdivided into multiple layers, of which the first layer is the most important one for our analysis.
The first layer classifies pipeline slots into one of four categories:
\begin{enumerate}
    \item \textbf{Retiring}: Pipeline slots where micro-operations complete successfully and contribute to useful work.
    \item \textbf{Bad Speculation}: Slots wasted due to branch mispredictions or other speculative execution that was later discarded.
    \item \textbf{Frontend Bound}: Slots where the backend could accept work, but the frontend (instruction fetch and decode) cannot supply micro-operations fast enough.
    \item \textbf{Backend Bound}: Slots where the frontend has work ready, but the backend (execution units and memory subsystem) cannot accept it due to resource contention or memory latency.
\end{enumerate}
As expected, each benchmark shows a significant increase in the number of instructions.
Most of \rtwoc's diversification measures, \eg prolog trap insertion, \code{NOP} insertion, \glspl{BTRA} and \glspl{heapbt}, require additional instructions.
Additional instructions can affect execution performance in at least two ways:
\begin{enumerate*}
    \item Executing an instruction requires \gls{CPU} cycles.
    This effect is particularly pronounced in hot code;
    \item Additional instructions change the code geometry and increase instruction cache pressure.
    Thus, additional instructions can influence other code parts, even if they are not executed at all.
\end{enumerate*}
Conversely, additional instructions do not necessarily lead to more cycles, as sometimes they can hide in stalls.
The relationship between an increase in instructions and an increase in cycles (if any), thus, can give us a first hint at a benchmark's performance profile.
Given that \glspl{BTRA} are responsible for most of the additional instructions, we performed a series of experiments with all \rtwoc's defenses in place, but a varying number of \glspl{BTRA}, ranging from 4 to 24.
\cref{r2c:tab:btra-steps-overhead} shows the performance overhead of each progression relative to the baseline.
\Cref{r2c:tab:ipc-progression} shows the absolute \gls{IPC} values as the number of \glspl{BTRA} increases from 2 to 22.
\cref{r2c:fig:btra-instr-vs-cycles} shows the relation between the cycle overhead and instruction overhead for increasing numbers of \glspl{BTRA}, relative to the baseline.
In other words, \cref{r2c:fig:btra-instr-vs-cycles} is a graphical depiction of the numerical values in \cref{r2c:tab:ipc-progression}.
\begin{table}[t]
    \centering
    \caption{\gls{IPC} progression with increasing \gls{BTRA} count.
    Benchmarks with memory slack show \gls{IPC} improvement as \gls{BTRA} instructions fill idle cycles; \gls{CPU}-bound benchmarks show flat or slightly decreasing \gls{IPC}.}
    \label{r2c:tab:ipc-progression}
    \footnotesize
    \begin{tabular}{l
    S[table-format=1.2]
        c c c c c c}
        \toprule
        & & \multicolumn{6}{c}{\glspl{BTRA} ($\Delta$\%)} \\
        \cmidrule(lr){3-8}
        Benchmark & {Baseline} & {2} & {6} & {10} & {14} & {18} & {22} \\
        \midrule
        \fileInput{generated/r2c/ipc-progression}
        \bottomrule
    \end{tabular}
\end{table}
\propername{Perlbench}, for example, shows an almost perfectly linear correlation, meaning that each additional instruction also requires a proportional number of additional cycles.
\propername{Perlbench} already has a relatively high \gls{IPC} of ~2.5 in the baseline, meaning that \propername{perlbench} utilizes the \gls{CPU}'s pipeline well and there is little room for additional instructions to hide.
We initially speculated that \propername{perlbench}, just like \propername{xalancbmk}, is affected by an increased instruction-cache pressure.
This hypothesis was supported by the sharp increase in L1 (+72\%) and L2 (+90\%) code cache misses for \propername{perlbench} in the full \rtwoc configuration.
The relative numbers can be misleading, however, as a comparison of the L1 and L2 miss rates and a relatively low increase in L2 stall cycles suggests that the L2 cache absorbs most of the L1 misses and the L3 cache most of the L2 misses.
The positive effect of the cache hierarchy is also confirmed by the \gls{TMA} counters: \rtwoc causes no notable increase in the \propername{Frontend Bound} category.
No increase in the \propername{Frontend Bound} category means that with \rtwoc, the \gls{CPU} does not stall more on waiting for instructions than it already did in the baseline.
Additional cache misses and memory stalls in \propername{perlbench} cause overall only about 1.5\% of the additional cycles.
The remaining cycle overhead is caused by additional instructions, which cannot hide in the relatively small stall room of the baseline.
For benchmarks below the diagonal, an additional instruction takes less than one cycle, on average.
This phenomenon can happen, for example, through memory stalls.
Memory stalls enable the \gls{CPU} to execute other instructions in the speculation window \enquote{for free}, while waiting for the fulfillment of the memory request.
Whether an instruction can hide in such a stall window depends on whether the instruction lands in a speculation window including a stall.
For example, if the \glspl{BTRA} setup sequence is preceded by a stalling instruction, the \glspl{BTRA} execute essentially for free (not accounting for stalls the \glspl{BTRA} themselves might cause).
\cref{r2c:fig:stall-hiding} illustrates this principle.
The instructions within an out-of-order execution window can execute before their program-order predecessors and, if they share the window with a stalling instruction, potentially during the stall cycles.
The stall cycles of window \propername{W\textsubscript{2}}, for example, are underutilized as there are no instructions that can execute during this stall.


\begin{table}[t]
    \centering
    \caption{Call density analysis. Call counts are from runtime instrumentation (see \cref{tab:call-frequencies}); baseline instructions from \texttt{perf stat}. Density is calls per 1000 baseline instructions; higher density means more \gls{BTRA} overhead per unit of work.}
    \label{r2c:tab:call-density}
    \sisetup{
        table-alignment-mode=none,
        table-number-alignment=right,
    }
    \begin{tabular}{l
        S[table-format=4.0]
        S[table-format=2.1]
        S[table-format=2.1]}
        \toprule
        Benchmark              & {Baseline Instr (B)} & {Calls (B)} & {Density} \\
        \midrule
        \propername{x264}      & 867                  & 3.4         & 3.9       \\
        \propername{deepsjeng} & 1957                 & 11.4        & 5.8       \\
        \propername{leela}     & 1638                 & 13.1        & 8.0       \\
        \propername{perlbench} & 994                  & 9.4         & 9.5       \\
        \propername{gcc}       & 781                  & 7.5         & 9.6       \\
        \propername{xalancbmk} & 1053                 & 12.4        & 11.8      \\
        \propername{mcf}       & 1965                 & 38.7        & 19.7      \\
        \propername{omnetpp}   & 1020                 & 23.5        & 23.1      \\
        \bottomrule
    \end{tabular}
\end{table}

If no such stalling instruction exists in the \glspl{BTRA}'s proximity, the \glspl{BTRA} execution delays the execution of other instructions.
In benchmarks with memory stalls, thus, \glspl{BTRA} cost fewer cycles on average.
For example, for \propername{mcf} we observe that while, relative to the baseline, instructions increase by almost 42\%, cycles increase only by about 13\%.
This indicates that stalls can absorb many of the additional instructions, explaining at least in part the relatively low overhead of \propername{mcf}.
\propername{Omnetpp} shows a similar picture, with instructions increasing by 42\%, but cycles increasing only by about 16\%.
The memory hiding theory is further substantiated by the fact that for both \propername{mcf} and \propername{omnetpp} the \propername{Retiring} category \emph{increases}.
Such an increase means that the CPU performs more work (presumably the \glspl{BTRA} instructions) relative to the overall cycles instead of \eg waiting for a memory stall.

\propername{Gcc} shows a combination of the phenomena discussed so far.
Like for \propername{perlbench}, the cache hierarchy absorbs the additional instruction pressure well, causing no notable increase in the \propername{Frontend Bound} category.
Like \propername{mcf} and \propername{omnetpp}, the \propername{gcc} baseline provides some stall room for additional instructions to hide, although the effect is less pronounced than for \propername{mcf} and \propername{omnetpp}.
Additional instructions that cannot hide in the fewer stall cycles, thus, directly contribute to \propername{gcc}'s overhead.

For \propername{deepsjeng}, which is also below the diagonal but closer to it than \eg \propername{omnetpp} and \propername{mcf}, the hiding effect is also less pronounced.
In particular, \rtwoc causes about 12\% more instructions but only 9\% more cycles.
The relatively low performance overhead of \propername{deepsjeng} is better explained by its low \emph{call density}, \ie the ratio of calls to baseline instructions.
\Cref{r2c:tab:call-density} shows that \propername{x264} has the lowest call density (3.9 calls per 1000 instructions) due to its loop-intensive video encoding kernels, followed by \propername{deepsjeng} and \propername{leela} (5.8 and 8.0 respectively) which execute about 170 and 125 instructions between calls on average.
In contrast, \propername{mcf} and \propername{omnetpp} call functions every 50 and 43 instructions respectively, resulting in call densities of 19.7 and 23.1.
Since each call instrumented by \rtwoc adds approximately 25 \gls{BTRA} instructions, the instruction overhead is roughly proportional to call density.
This explains why \propername{deepsjeng} has only 12\% instruction overhead despite executing more total calls than \propername{perlbench} (11.4B vs 9.4B): the fixed \gls{BTRA} cost is amortized over more baseline work.

\propername{xalancbmk}, which shows a similar \gls{IPC} progression in \cref{r2c:fig:btra-instr-vs-cycles} as \propername{deepsjeng}, can also hide some of the additional instructions in stalls.
Specifically, \propername{xalancbmk} with \rtwoc has 23\% additional instructions, but only 12\% additional cycles.
Like for other benchmarks below the diagonal, we can see an increase in the \propername{Retiring} category and a decrease in the \propername{Backend Bound} category, meaning that preexisting stalls can hide some of the additional instructions.
The L1/L2/L3 instruction cache counters also indicate that the cache hierarchy can absorb much of the additional instruction pressure.
Unlike with the other benchmarks, however, we see a slight increase (+1.2\%) in the \propername{Frontend Bound} category, suggesting that the additional instruction pressure occasionally overwhelms the caches.

The benchmark \propername{x264} shows a small amount of hiding.
The actual cycle overhead is about 30\% smaller than the instruction overhead alone predicts.
That is, about 30\% of the additional cycles can hide in a preexisting stall.
The overall low overhead is best explained by the low call density of x264 (see \cref{r2c:tab:call-density}).
Already in the baseline the \gls{CPU} spends most of its time performing time-consuming number crunching for the video encoding.

\propername{Leela}'s low performance overhead is likely the result of execution port underutilization rather than memory slack alone.
The baseline \gls{IPC} is low ($\approx$1.05), indicating that a 4-wide core leaves substantial headroom for additional instructions to execute in parallel.
The perf counter data also show non-trivial memory stalls (about 22\%) and divider activity (about 3\%), which provide additional hiding capacity.
Both \propername{Backend Bound} and the \propername{Frontend Bound} categories decrease slightly with \rtwoc, consistent with BTRA instructions filling otherwise idle slots.
Notably, \propername{leela} has the highest \propername{Bad Speculation} rate among our benchmarks ($\approx$22\%), which explains why its baseline \gls{IPC} is low despite not being memory-bound---the CPU frequently discards speculative work.
However, this misprediction overhead does not itself create hiding capacity; rather, it is the resulting execution port underutilization that provides slack for \gls{BTRA} instructions to execute without extending the critical path.
\subsubsection{Memory Overhead}
Surprised by the difference of memory overhead between SPEC CPU 2017 benchmarks and webserver benchmarks, we verified the memory SPEC results by applying the same methodology as for the webserver benchmarks.
Instead of relying on the \propername{maxrss} counter, we recorded the RSS usage of the SPEC benchmarks with a separate monitoring process.
The results confirmed a memory overhead of only a few percent.
We suspect that for the SPEC benchmarks, memory overhead caused by \rtwoc{} is low compared to the memory consumed by the benchmark itself.
Further research is necessary to substantiate these suspicions.

%The overhead of \glspl{heapbt} and \glspl{BTRA} is mostly independent.
%Only in the case of \propername{omnetpp} is the combined overhead of \glspl{heapbt} and the AVX setup sequence bigger than the sum of the component overheads.
%The overhead of \glspl{heapbt} varies less per benchmark, whereas \glspl{BTRA} affect different benchmarks to different degrees.
%These changes in overhead can be attributed to the fact that \glspl{BTRA} are inserted \emph{per call site}.
%As a result, benchmarks with a large number of function calls are affected more by \glspl{BTRA}.
% Interestingly, despite using the hardened pointer array (see~\ref{r2c:ss:impl-heap-boobytraps}), the variant \propername{AVX2+H\heapbt} shows a slightly improved performance on three benchmarks compared to \propername{AVX2+\heapbt}.
% We suspect an improved cache locality of the pointer array with other data to be responsible for the improvement.

%The performance difference on our benchmark environment illustrated in \Cref{r2c:fig:perf-full} warranted further investigation.
%To account for the randomness invariably introduced by the diversification of the individual benchmark programs, we generated 25 variants of each program and measured their runtime performance impact.
%\Cref{r2c:fig:intra-variant-difference} shows these data and indicates that the performance can depend significantly on the diversification process.
%If users require peak performance, compiling multiple variants and selecting the one with the lowest performance impact is a viable option.

\subsection{Security}\label{r2c:ss:discussion-security}
While execute-only memory and function shuffling defeat classic ROP and JIT-ROP attacks, indirect JIT-ROP and \gls{AOCR} remain an issue.
For an indirect JIT-ROP attack, an attacker needs to locate valid code pointers, such as return addresses, in readable memory and infer gadget locations based on the found pointers.
For an \gls{AOCR} attack, an attacker needs to locate function pointers or infer them from other code pointers (\eg return addresses), as well as manipulate function parameters.
In the following subsections we discuss how \rtwoc{} counters each of these attack vectors.
We also discuss the security of stack unwinding tables and the possibility of an attacker corrupting entire or partial code pointers.

Our security evaluation relies on a probabilistic model to estimate an attacker's success rate.
We instantiate the model's variables (such as stack frame composition and pointer distribution) with empirical measurements from \propername{nginx}, \propername{Apache}, and \propername{Lighttpd}.

\subsubsection{Leaking Return Addresses}\label{r2c:sss:leaking-return-addresses}
\rtwoc protects return addresses with \glspl{BTRA}.
As detailed in \cref{r2c:ss:decoy-mimicry}, the only way for an attacker to identify a return address among the \glspl{BTRA} is by applying brute force.
An attacker's chance to correctly guess the return address depends on the number of \glspl{BTRA} used.
If $B$ is the number of \glspl{BTRA} for a call site, the probability of correctly guessing the return address is given by
\[ P(\mathrm{return address})=\frac{1}{B+1}. \]
Specifically, with 11 \glspl{BTRA} per return address $P(\mathrm{return address})\approx0.09$.
Since return addresses on their own are of limited use, they form only a building block for further inferences.

\subsubsection{Leaking Function Pointers}\label{r2c:sss:leaking-function-pointers}
For an \gls{AOCR}-style whole-function-reuse attack, an attacker needs to locate \emph{specific} function pointers.
That is, the attacker must locate a function pointer with manipulatable parameters and desired functionality.
There are three ways for an attacker to locate function pointers:
\begin{enumerate*}
    \item from the data section;
    \item via the stack, either from function pointers on the stack or indirectly from return addresses
    \item from the heap.
\end{enumerate*}

\rtwoc does not directly protect the heap against information leakage, but aims to prevent an attacker from reaching the heap via the stack via \glspl{heapbt}.
We discuss their respective security properties in \cref{r2c:sss:heap-data}.
\gls{ASLR} randomizes the address of the data section and global-variable shuffling randomizes the order of elements in the data section.
Leaking a function pointer from the data section, therefore, requires a leaked data section pointer first.
Within the data section, an attacker must then locate the desired function pointer.
Exact localization is challenging because of function permutation and boobytrap pointers in the data section.

Leaking a function pointer from the stack can happen either directly or indirectly via a previously leaked return address.
We start with the probability of a direct leak.
Estimating the success probability of locating function pointers on the stack depends on the number of function pointers per stack frame.
Assuming that an attacker can leak arbitrary stack snapshots (see \cref{r2c:s:threat-model}) and can differentiate stack frames within a stack snapshot, stack slot shuffling is currently the only protection.
An attacker can reasonably differentiate function pointers from the return address and \glspl{BTRA} because these occur only at the top of a frame.
With value range analysis an attacker can further differentiate function pointers from other stack slots, such as variables values or heap pointers.
Thus, guessing a function pointer's location means choosing an element from a permutation of all function pointers in the stack frame.
To measure the probability empirically, we have collected stack snapshots from benchmarking workloads on \propername{Apache}, \propername{nginx}, and \propername{Lighttpd}.
\cref{r2c:tab:func-ptr-dist} shows the function pointer distribution in these snapshots.
About 25\% to 26\% of stack frames (depending on the software) contain no function pointer at all.
In \propername{nginx}, 45\% of all stack frames contain only one function pointer, in \propername{Apache} 35\%, and in \propername{Lighttpd} 14\%.
These cases are troublesome because they mean that an attacker can immediately identify the desired pointer.
Even with two function pointers per frame (between 4.5\% and 30.5\%) the probability of guessing correctly is still 50\%.
We note, however, that only very select function pointers are suitable for \gls{AOCR}'s variant of whole-function-reuse attack and that global variable shuffling further frustrates efforts to locate and manipulate default function parameters.


\begin{table}[t]
    \centering
    \caption{Empirical distribution of function pointers per stack frame.}
    \label{r2c:tab:func-ptr-dist}
    \begin{tabular}{c rr rr rr}
        \toprule
        & \multicolumn{2}{c}{\propername{nginx}} & \multicolumn{2}{c}{\propername{Apache}} & \multicolumn{2}{c}{\propername{Lighttpd}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        Count & Frames & \%   & Frames & \%   & Frames & \%   \\
        \midrule
        0     & 74     & 18.5 & 266    & 54.4 & 96     & 25.3 \\
        1     & 174    & 43.5 & 193    & 39.5 & 99     & 26.1 \\
        2     & 122    & 30.5 & 22     & 4.5  & 48     & 12.6 \\
        3     & 23     & 5.8  & 3      & 0.6  & 27     & 7.1  \\
        4     & 5      & 1.3  & 2      & 0.4  & 18     & 4.7  \\
        5     & 2      & 0.5  & 2      & 0.4  & 12     & 3.2  \\
        6     & 0      & 0.0  & 1      & 0.2  & 6      & 1.6  \\
        7     & 0      & 0.0  & 0      & 0.0  & 9      & 2.4  \\
        8     & 0      & 0.0  & 0      & 0.0  & 6      & 1.6  \\
        9     & 0      & 0.0  & 0      & 0.0  & 6      & 1.6  \\
        10    & 0      & 0.0  & 0      & 0.0  & 3      & 0.8  \\
        11+   & 0      & 0.0  & 0      & 0.0  & 50     & 13.2 \\
        \bottomrule
    \end{tabular}
\end{table}

Leaking a function pointer from a return address requires more guessing.
Let us assume that an attacker has successfully leaked a return address $\mathcal{A}$.
Let us further assume that the attacker \emph{knows} that $\mathcal{A}$ points into a function $\mathcal{F}$.
The attacker can then try to guess the entrypoint of $\mathcal{F}$ for a whole-function-reuse attack.
The function's entrypoint is guaranteed to be before $\mathcal{A}$.
Assuming an average function size of $S$, there are on average $\frac{S}{2}$ addresses before a random return site within $\mathcal{F}$.
With a function alignment of $A$ (typically 16), there are, therefore, on average $\frac{S}{2A}$ possible entry points.
\rtwoc{}'s additional code randomization (see \cref{r2c:ss:strengthening}) decreases the success probability further.
With inserted traps, the function's start address does not coincide with the function's entrypoint.
With $T$ inserted trap instructions, an attacker must guess $\frac{S}{2*A} * T$ possible entrypoints.
The probability of correctly guessing the function's entrypoint thus becomes
\[ P(\mathrm{entrypoint})=\frac{2*A}{S*T}. \]
The average function size $S$ depends on the program, the alignment is typically 16, and the average number of traps depends on a configurable parameter in \rtwoc.
For an alignment $A=16$, a median function size of $S=258$ bytes in a protected \propername{nginx} and an average number of prolog traps $T=3$, this means $P(\mathrm{entrypoint})\approx0.04$.
Estimating based on the median function size is conservative, as the average function size in \propername{nginx} is 789 bytes.
The probability for leaking a single function pointer based on a leaked return address is
\[ P(\mathrm{retaddr\rightarrow{}func}) = P(\mathrm{return address}) * P(\mathrm{entrypoint}). \]
With the concrete numbers from above $P(\mathrm{retaddr\rightarrow{}func})\approx0.0038$.

\subsubsection{Leaking Gadget Addresses}
Instead of mounting a whole-function-reuse attack, an attacker can try to leak enough gadget addresses for a \gls{ROP} attack.
Except for completely blind guessing, an attacker can guess gadget addresses relative to an anchor point.
Such an anchor point can either be a leaked function address (see \cref{r2c:sss:leaking-function-pointers}) or a leaked return address (see \cref{r2c:sss:leaking-return-addresses}).
In either case, the attacker knows the number of call sites between the anchor and the gadget of choice, since \rtwoc does not change the number of call sites.
Each call site has $R_{B}$ \glspl{BTRA} and for each \gls{BTRA}, \rtwoc adds a \code{NOP} with a probability of $P_{NOP}$.
The number of \glspl{BTRA} and the probability of inserted \code{NOP}s are configurable parameters in \rtwoc.
Let $C$ be the average number of call sites within a function in the target program.
We can then estimate the number of call sites between the anchor and a gadget address as $C_{between}=\frac{C}{2}$ on average.
$C_{between}$ is, of course, only a rough estimate that highly depends on $C$.
For \propername{nginx}, we found $C$ to be 4 (median) and 7.3 (average).
Based on these variables, we derive that between the anchor and the gadget, there are $C_{between} * R_{B} * P_{NOP}$ \code{NOP}s on average.
The probability of guessing a gadget address thus becomes
\[ P(\mathrm{gadget})=\frac{1}{C_{between}*B*P_{NOP}}. \]
With 11 \glspl{BTRA} per return address, a probability of $P_{NOP}=0.35$ and $C_{between}=2$, which we used for full \rtwoc, we get $P(\mathrm{gadget})\approx0.13$.
Estimating based on the median number of call sites is conservative, as the average number of call sites per function in \propername{nginx} is 7.
Gadget-based attacks are further impeded by register-allocation randomization.

An attacker typically needs more than one gadget, further decreasing the chance of successfully guessing \emph{all} locations.
For example, the probability of leaking two return addresses and within each function two gadgets is
\[ P\left(\mathrm{2retaddrs\rightarrow{}4gadgets}\right) = P(\mathrm{return address})^2 * P(\mathrm{gadget})^4. \]
With the concrete numbers from above this means $P\left(\mathrm{2retaddrs\rightarrow{}4gadgets}\right)\approx0,00017$.

\subsubsection{Leaking Heap Data}\label{r2c:sss:heap-data}
An attacker can try to leak data from the heap to either learn function pointers or pointers leading to the data section.
As \gls{ASLR} randomizes the heap's location, leaking data requires either a heap over-read vulnerability or a heap pointer.
Such a heap pointer can come either from the data section or the stack.
Unlike with function pointers, the attacker can pick an arbitrary heap pointer to reach the heap.
Heap pointers in the data section are rare, which is why we focus our analysis on the stack.
The probability of correctly guessing a benign heap pointer among all heap pointers depends on the number of benign heap pointers $H$, and the number of \glspl{heapbt} $H_{B}$ in a stack dump.
The number of $H_{B}$ in turn depends on the number of stack frames in the dump because \rtwoc inserts \glspl{heapbt} into each stack frame, depending on a configuration parameter (see \cref{r2c:ss:impl-heap-boobytraps}).
The probability of randomly picking a benign pointer is $\frac{H}{H_{B}+H}$.
The exact number for $H$ is application-specific and depends, for example, on the number of registers containing heap pointers that are spilled to the stack.
\cref{r2c:tab:heap-ptr-probabilities} shows the empirical distribution of heap pointers in stack snapshots taken from \propername{Apache}, \propername{nginx}, and \propername{Lighttpd} during benchmark workloads.
The table also shows the success probability of randomly guessing a heap pointer among all heap pointers found in a randomly chosen stack snapshot.
As the success probability depends on the concrete composition of the stack snapshot, the table shows the worst, average, and best case for the attacker.
Assuming that an attacker can dump arbitrary many stack snapshots (see \cref{r2c:s:threat-model}), the attacker can repeat the leak until she finds a best-case snapshot.
Generally, snapshots with few stack frames but many heap pointers in those frames constitute the best case for the attacker.
Since \rtwoc inserts a configurable number of \glspl{heapbt} \emph{per frame}, stack snapshots with fewer frames contain also fewer \glspl{heapbt}.
\cref{r2c:tab:heap-ptr-probabilities} also shows, however, that even in the worst case (for the attacker), the success probability still ranges between 25\% and 52.6\%.

The numbers above suggest that \glspl{heapbt} offer only limited protection against heap-based attacks.
We discuss a possible extension of the mimicry principle \emph{onto} the heap in \cref{r2c:ss:heap-mimicry}.

Alternatively to guessing a heap pointer, an attacker could try to identify events where \glspl{heapbt} do not mimic their benign counterparts accurately.
For example, by performing heap feng shui an attacker might be able to identify benign heap pointers with a known distance to each other~\cite{Sotirov2007}.
Note, however, that such an attack requires specific prerequisites and goes significantly beyond the analysis steps of the demonstrated \gls{AOCR} attacks.


\begin{table}[t]
    \centering
    \caption{\gls{heapbt} single-guess success probabilities for a random stack snapshot.
    Each stack frame contains three \glspl{heapbt} on average.}
    \label{r2c:tab:heap-ptr-probabilities}
    \begin{tabular}{ll rrr r}
        \toprule
        \textbf{Server} & \textbf{Case} & Depth & Heap Ptr. & \glspl{heapbt} & $P_{Success}$ \\
        \midrule
        \multirow{4}{*}{\propername{nginx}}
        & Worst         & 12    & 40        & 36             & 52.6\%        \\
        & Average       & 22    & 100       & 66             & 60.2\%        \\
        & Median        & 22    & 102       & 66             & 60.7\%        \\
        & Best          & 15    & 74        & 45             & 62.2\%        \\
        \midrule
        \multirow{4}{*}{\propername{Apache}}
        & Worst         & 174   & 194       & 522            & 27.1\%        \\
        & Average       & 140   & 269       & 420            & 39.1\%        \\
        & Median        & 140   & 273       & 420            & 39.4\%        \\
        & Best          & 4     & 29        & 12             & 70.7\%        \\
        \midrule
        \multirow{4}{*}{\propername{Lighttpd}}
        & Worst         & 4     & 4         & 12             & 25.0\%        \\
        & Average       & 5     & 48        & 15             & 71.8\%        \\
        & Median        & 8     & 87        & 24             & 78.4\%        \\
        & Best          & 7     & 116       & 21             & 84.7\%        \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/r2c/ehframes}%
    \caption{Without randomization, an attacker can infer structural information about functions, such as the relative location of the return address from the Call-Frame Information directives in the \code{eh\_frame} section.
        \rtwoc's (right) randomization measures break the inference step from information in the \code{.eh\_frame} section to functions.}
    \label{r2c:fig:ehframes}
\end{figure}


\begin{table}[t]
    \centering
    \caption{Function pointer single-guess success probabilities for a random stack frame.
    Each stack frame contains three fake function pointers.}
    \label{r2c:tab:func-ptr-probabilities}
    \begin{tabular}{ll rr r}
        \toprule
        \textbf{Server} & \textbf{Case} & Real Func. Ptr. & Total Func. Ptr. & $P_{Success}$ \\
        \midrule
        \multirow{4}{*}{\propername{nginx}}
        & Worst         & 24              & 27               & 3.7\%         \\
        & Average       & 2.8             & 5.8              & 17.3\%        \\
        & Median        & 1               & 4                & 25.0\%        \\
        & Best          & 1               & 4                & 25.0\%        \\
        \midrule
        \multirow{4}{*}{\propername{Apache}}
        & Worst         & 18              & 21               & 4.8\%         \\
        & Average       & 1.5             & 4.5              & 22.0\%        \\
        & Median        & 1               & 4                & 25.0\%        \\
        & Best          & 1               & 4                & 25.0\%        \\
        \midrule
        \multirow{4}{*}{\propername{Lighttpd}}
        & Worst         & 72              & 75               & 1.3\%         \\
        & Average       & 19.9            & 22.9             & 4.4\%         \\
        & Median        & 2               & 5                & 20.0\%        \\
        & Best          & 1               & 4                & 25.0\%        \\
        \bottomrule
    \end{tabular}
\end{table}

Given that \glspl{heapbt} incur overhead but affords only limited protection, an alternative use of the underlying implementation could be to write fake code pointers instead.
\glspl{BTRA} already provide the means for code pointers pointing to booby traps and the \glspl{heapbt} implementation way of writing pointers to random stack slots.
The expected overhead would remain the same or be less because the heap is no longer fragmented by guard pages.
In a function-pointer-guessing scenario, however, the afforded security of pointer mimicry is better.
Recall from \cref{r2c:sss:leaking-function-pointers} that when leaking a function pointer, the attacker needs a \emph{specific} pointer within a \emph{specific} stack frame.
Pointer mimicry could thus help in cases where there are only few function pointers in a stack frame and, as a result, stack slot randomization does not sufficiently protect them (see \cref{r2c:tab:func-ptr-dist}).
Analogous to \cref{r2c:tab:heap-ptr-probabilities}, \cref{r2c:tab:func-ptr-probabilities} contains empirical data together with success probabilities for guessing a function pointer, assuming three fake function pointers per frame on average.
Note that unlike \cref{r2c:tab:heap-ptr-probabilities}, \cref{r2c:tab:func-ptr-probabilities} shows data \emph{per frame}.
Whereas without fake function pointers the best case success probability for the attacker was 100\%, the probability with fake pointers drops to 25\%.
Care must be taken, however, not to write fake code pointers into stack frames without real function pointers.
Since an attacker knows which stack frames contain function pointers in the non-randomized binary, the attacker could identify fake pointers in frames that previously did not contain any function pointers.

\subsubsection{Exception handling and stack unwinding}
As part of the \gls{BTRA} setup and teardown code, \rtwoc{} also emits the necessary Call-Frame Information directives to support exception handling and stack unwinding.
Call-Frame Information directives record stack pointer and frame modifications in the \code{.eh\_frame} section.
Since the modifications are recorded relative to the beginning of the frame, decoding the \code{.eh\_frame} section could reveal the location of the return address.
Entries in the \code{.eh\_frame} are not, however, associated with function symbols, but with \gls{PC} \emph{ranges}.
These \gls{PC} ranges are unknown to the attacker due to code layout randomization.
An attacker cannot, therefore, associate entries in the \code{.eh\_frame} table with functions.
\cref{r2c:fig:ehframes} shows an example of the inference before and after \rtwoc.

The position of an entry in the table---i.e., its row---could provide the attacker with important information.
Each entry in the table, reflects the position of a function in a compilation unit.
% Through function reordering/permutation, the attacker is also deprived of this information.
Through function reordering/permutation row-based references become invalid.
Since exceptions occur infrequently, one could also use a more expensive protection scheme, such as encryption, to protect these meta-data.

\subsubsection{Corrupting code pointers}\label{r2c:sss:corrupting-code-pointers}
\Glspl{CRA} typically corrupt entire code pointers.
An attack called \gls{PIROP} generalizes this principle by corrupting only parts of code pointers and, as a result, is immune to ASLR and page-level randomization~\cite{Goktas2018}.
\rtwoc{} impedes a \gls{PIROP} attack in two ways.
First, \rtwoc{} shuffles functions and randomizes at the sub-function level (see \cref{r2c:ss:strengthening}), thus increasing the entropy for \gls{PIROP}.
Second, \glspl{BTRA} constrain candidate \gls{PIROP} gadgets that manipulate (partial) return addresses:
In the presence of \glspl{BTRA} a \gls{PIROP} attack needs to corrupt \emph{all} return addresses, requiring either iterative gadget execution or more complex gadgets.

\subsection{Remaining attack surface}\label{r2c:ss:remaining-attack-surface}
At present, \rtwoc{} remains susceptible to two types of brute force attacks.
In a Blind ROP scenario with restarting worker processes, an attacker could use \gls{PIROP} to brute force the entropy resulting from \rtwoc{} randomization techniques.
Similarly, an attacker could use the corruption of potential return addresses as a side channel.
For example, by overwriting selected return address candidates with zero and observing whether the process crashes, the attacker could learn the location of the real return address.
Both attacks could be prevented by load time re-randomization.

\rtwoc{} could also deter the corruption of \glspl{BTRA} by checking a random subset of \glspl{BTRA} for consistency after the return.

\rtwoc{} focuses on code-reuse attacks and the leakage of pointers that enable \emph{control-flow} hijacking.
Although \rtwoc{}'s layout randomization raises the bar for attackers~\cite{Hu2016}, \rtwoc{} does not offer the same protection as defenses specialized for data-only attacks~\cite{Carr}.

A way to strengthen \rtwoc{}'s security would be to combine it with \gls{MVEE}s~\cite{Cox2006,Berger2006,Bruschi2007,Volckaert2016,Voulimeneas2020}.
\gls{MVEE}s and diversification defenses like \rtwoc{} naturally complement each other.
Considering that \rtwoc{} diversifies along multiple dimensions, an \gls{MVEE} would detect data corruption or leakage in one of the variants with high probability.
