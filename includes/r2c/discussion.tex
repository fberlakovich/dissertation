\section{Discussion}\label{sec:r2c:discussion}
%\begin{figure}[t!]
%  \centering
%  \input{stats/20220519-boxplot-variance-diff-horizontal.pgf}
%  \caption{Performance impact variance of differently diversified variants on selected benchmarks on our \textsf{TR 3970X} A system.}
%  \label{r2c:fig:intra-variant-difference}
%\end{figure}

\subsection{Performance}\label{r2c:ss:discussion-evaluation}

\fbetodo{Add perf counter discussion}
\fbetodo{Add section about profile-guided optimization}
\begin{table}[t]
    \sisetup{
        table-alignment-mode=none,
        table-number-alignment=right,
    }
    \begin{tabular}{lS[table-format=8.0]}
        \toprule
        Benchmark              & {Call Frequency} \\
        \midrule
        \propername{perlbench} & 9435182963       \\
        \propername{gcc}       & 7471474392       \\
        \propername{mcf}       & 38657893688      \\
        \propername{lbm}       & 20906700         \\
        \propername{omnetpp}   & 23536583520      \\
        \propername{xalancbmk} & 12430137048      \\
        \propername{x264}      & 3400115007       \\
        \propername{deepsjeng} & 11366032234      \\
        \propername{imagick}   & 10441212712      \\
        \propername{leela}     & 13108456661      \\
        \propername{nab}       & 135237228510     \\
        \propername{xz}        & 3287645643       \\
        \bottomrule
    \end{tabular}
    \caption{Median call frequencies of SPEC CPU 2017 benchmarks across all inputs.}
    \label{tab:call-frequencies}
\end{table}


\begin{table}[t]
    \centering
    \caption{\Gls{BTRA} overhead by step count. Cycles column shows overhead in percent.}
    \label{tab:btra-overhead}
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l *{6}{rr}}
        \toprule
        & \multicolumn{2}{c}{2 BTRAs} & \multicolumn{2}{c}{6 BTRAs} & \multicolumn{2}{c}{10 BTRAs} & \multicolumn{2}{c}{14 BTRAs} & \multicolumn{2}{c}{18 BTRAs} & \multicolumn{2}{c}{22 BTRAs} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
        Benchmark & Cycles & IPC & Cycles & IPC & Cycles & IPC & Cycles & IPC & Cycles & IPC & Cycles & IPC \\
        \midrule
        \fileInput{generated/r2c/btra-step-cycles}
        \bottomrule
    \end{tabular}
\end{table}

As evidenced by the benchmarks, the optimized \gls{BTRA} setup sequence improves \rtwoc{}'s performance considerably (see \cref{r2c:fig:perf-components}).
While our implementation uses AVX2 vector instructions, falling back to SSE vector instructions would be an alternative for more feature constrained CPUs.
For CPU's without vector extensions the \code{push} based setup sequence provides a viable alternative without loss of security.
The difference between the \code{push} and AVX2 setup sequence (see \cref{r2c:fig:perf-components}) indicates that increased instruction cache pressure contributes to the overhead.
Similarly, prolog trap insertion also contributes to the increased instruction cache pressure.

% We speculate that at the other end of the spectrum an implementation with AVX512 registers could improve performance even more.
% We could not evaluate this effect due to the lack of compatible hardware.

\Cref{r2c:fig:perf-full} shows that benchmarks with a large number of functions and function calls are affected most by \rtwoc{}.
\rtwoc{} adds \glspl{BTRA} \emph{per call site}, explaining the overhead for function heavy benchmarks.
To test this correlation with call frequency, we instrumented the SPEC CPU benchmark programs to count the number of executed call instructions.
Our instrumentation ignores tail calls because tail calls do not push a return address to the stack and, thus, no \glspl{BTRA} are inserted.
\Cref{tab:call-frequencies} shows the median number of calls performed by the SPEC CPU 2017 benchmarks.
For each benchmark we took the median call frequencies across all inputs.
The data suggests that there is only a weak direct correlation between calls and the overhead:
\propername{Perlbench}, for example, has less than half the number of calls as \propername{omnetpp}, but shows a similar overhead.
The mere call count, therefore, does not sufficiently explain the performance profile.

A possible explanation for the super-additive overhead (sum of component-overheads is less than actual total overhead) in \propername{mcf} and \propername{omnetpp} are memory stalls.
Both \propername{mcf} and \propername{omnetpp} are memory-bound benchmarks and memory stalls can hide a certain number of additional instructions.
Thus, the stalls might hide the instructions of each component in isolation, but the stalls might be too few to hide all components combined.

\subsubsection{Detailed Performance Counter Analysis}
\begin{figure}[t]
    \centering
    \begingroup
    \pgfplotsset{
        every axis/.append style={
            yticklabel style={font=\footnotesize},
            xticklabel style={font=\footnotesize},
            execute at begin axis={
                \pgfplotsset{
                    width=\textwidth,
                    height=0.8\linewidth}
            },
        },
    }
    \input{figures/r2c/btra-instr-vs-cycles}%
    \endgroup
    \caption{The relationship between instruction overhead and cycle overhead.}
    \label{r2c:fig:btra-instr-vs-cycles}
\end{figure}

The example of call count and performance overhead differences between \propername{perlbench} and \propername{omnetpp} above showed that instructions and call counts alone give an incomplete picture.
To better understand the exact causes of the overhead, we recorded \code{perf} counters for particularly affected benchmarks on \propername{i9}.
We choose \propername{i9} instead of \propername{EPYC} for the \code{perf} analysis because of the richer set of performance counters on the Intel machine.
As expected, each benchmark shows a significant increase in the number of instructions.
Most of \rtwoc's diversification measures, \ie prolog trap insertion, \code{NOP} insertion, \glspl{BTRA} and \glspl{heapbt}, require additional instructions.
Additional instructions can affect execution performance in at least two ways:
\begin{enumerate*}
    \item Executing an instruction requires \gls{CPU} cycles.
    This effect is particularly pronounced in hot code;
    \item Additional instructions change the code geometry and increase instruction cache pressure.
    Thus, additional instructions can influence other code parts, even if they are not executed at all.
\end{enumerate*}
Conversely, additional instructions do not necessarily lead to more cycles, as sometimes they can hide in stalls.
The relationship between an increase in instructions and an increase in cycles (if any), thus, can give us a first hint of a benchmark's performance profile.
Given that \glspl{BTRA} are responsible for most of the additional instructions, we performed a series of experiments with all of \rtwoc's defenses in place, but a varying number of \glspl{BTRA}, ranging from 4 to 24.
\cref{r2c:fig:btra-instr-vs-cycles} shows the relation between the cycle overhead and instruction overhead for increasing numbers of \glspl{BTRA}.
\propername{Perlbench}, for example, shows an almost perfectly linear correlation, meaning that each additional instruction also requires a proportional number of additional cycles.

For benchmarks below the diagonal, an additional instruction takes less than one cycle, on average.
This phenomenon can happen, for example, through memory stalls.
Memory stalls enable the \gls{CPU} to execute other instructions in the speculation window \enquote{for free}, while waiting for the fulfillment of the memory request.
Whether an instruction can hide in such a stall window depends on whether the instruction lands in a speculation window including a stall.
For example, if the \glspl{BTRA} setup sequence is preceded by a stalling instruction, the \glspl{BTRA} execute essentially for free (not account for stalls the \glspl{BTRA} themselves might cause).
If no such stalling instruction exists in the \glspl{BTRA}'s proximity, the \glspl{BTRA} execution delays the execution of other instructions.
In benchmarks with memory stalls, thus, \glspl{BTRA} cost fewer cycles on average.
For example, for \propername{mcf} we observe that while, relative to the baseline, instructions increase by almost 60\%, cycles increase only by about 17\%.
This indicates that stalls can absorb many of the additional instructions, explaining at least in part the relatively low overhead of \propername{mcf}.
\propername{Omnetpp} shows a similar picture, with instructions increasing by 60\%, but cycles increasing only by about 26\%.

For \propername{deepsjeng}, which is also below the diagonal but closer to it than \eg \propername{omnetpp} and \propername{mcf}, the hiding effect is less pronounced.
In particular, \rtwoc causes about 17\% more instructions but only 12\% more cycles.
The relatively low performance overhead of \propername{deepsjeng} is better explained by its low \emph{call density}, \ie the ratio of calls to baseline instructions.
\Cref{tab:call-density} shows that \propername{deepsjeng} has the lowest call density (7.0 calls per 1000 instructions) because it executes about 143 instructions between calls on average.
In contrast, \propername{mcf} and \propername{omnetpp} call functions every 43 and 36 instructions respectively, resulting in call densities of 23.5 and 27.5.
Since each call adds approximately 25 \gls{BTRA} instructions, the instruction overhead is roughly proportional to call density.
This explains why \propername{deepsjeng} has only 17\% instruction overhead despite executing more total calls than \propername{perlbench} (11.4B vs 9.4B): the fixed \gls{BTRA} cost is amortized over more baseline work.

\begin{table}[t]
    \centering
    \caption{Call density analysis. Density is calls per 1000 baseline instructions; higher density means more \gls{BTRA} overhead per unit of work.}
    \label{tab:call-density}
    \sisetup{
        table-alignment-mode=none,
        table-number-alignment=right,
    }
    \begin{tabular}{l
        S[table-format=4.1]
        S[table-format=2.1]
        S[table-format=2.1]}
        \toprule
        Benchmark              & {Baseline Instr (B)} & {Calls (B)} & {Density} \\
        \midrule
        \propername{deepsjeng} & 1635.1               & 11.4        & 7.0       \\
        \propername{perlbench} & 829.1                & 9.4         & 11.4      \\
        \propername{gcc}       & 651.7                & 7.5         & 11.5      \\
        \propername{xalancbmk} & 879.8                & 12.4        & 14.1      \\
        \propername{mcf}       & 1645.8               & 38.7        & 23.5      \\
        \propername{omnetpp}   & 857.3                & 23.5        & 27.5      \\
        \bottomrule
    \end{tabular}
\end{table}

Surprised by the difference of memory overhead between SPEC CPU 2017 benchmarks and webserver benchmarks, we verified the memory SPEC results by applying the same methodology as for the webserver benchmarks.
Instead of relying on the \propername{maxrss} counter, we recorded the RSS usage of the SPEC benchmarks with a separate monitoring process.
The results confirmed a memory overhead of only a few percent.
We suspect that for the SPEC benchmarks, memory overhead caused by \rtwoc{} is low compared to the memory consumed by the benchmark itself.
Further research is necessary to substantiate these suspicions.

%The overhead of \glspl{heapbt} and \glspl{BTRA} is mostly independent.
%Only in the case of \propername{omnetpp} is the combined overhead of \glspl{heapbt} and the AVX setup sequence bigger than the sum of the component overheads.
%The overhead of \glspl{heapbt} varies less per benchmark, whereas \glspl{BTRA} affect different benchmarks to different degrees.
%These changes in overhead can be attributed to the fact that \glspl{BTRA} are inserted \emph{per call site}.
%As a result, benchmarks with a large number of function calls are affected more by \glspl{BTRA}.
% Interestingly, despite using the hardened pointer array (see~\ref{r2c:ss:impl-heap-boobytraps}), the variant \propername{AVX2+H\heapbt} shows a slightly improved performance on three benchmarks compared to \propername{AVX2+\heapbt}.
% We suspect an improved cache locality of the pointer array with other data to be responsible for the improvement.

%The performance difference on our benchmark environment illustrated in \Cref{r2c:fig:perf-full} warranted further investigation.
%To account for the randomness invariably introduced by the diversification of the individual benchmark programs, we generated 25 variants of each program and measured their runtime performance impact.
%\Cref{r2c:fig:intra-variant-difference} shows these data and indicates that the performance can depend significantly on the diversification process.
%If users require peak performance, compiling multiple variants and selecting the one with the lowest performance impact is a viable option.

\subsection{Security}\label{r2c:ss:discussion-security}

\begin{table}[t]
    \centering
    \caption{Empirical distribution of function pointers per stack frame.}
    \label{r2c:tab:func-ptr-dist}
    \begin{tabular}{c rr rr rr}
        \toprule
        & \multicolumn{2}{c}{\propername{nginx}} & \multicolumn{2}{c}{\propername{Apache}} & \multicolumn{2}{c}{\propername{Lighttpd}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        Count & Frames & \%   & Frames & \%   & Frames & \%   \\
        \midrule
        0     & 74     & 18.5 & 266    & 54.4 & 96     & 25.3 \\
        1     & 174    & 43.5 & 193    & 39.5 & 99     & 26.1 \\
        2     & 122    & 30.5 & 22     & 4.5  & 48     & 12.6 \\
        3     & 23     & 5.8  & 3      & 0.6  & 27     & 7.1  \\
        4     & 5      & 1.3  & 2      & 0.4  & 18     & 4.7  \\
        5     & 2      & 0.5  & 2      & 0.4  & 12     & 3.2  \\
        6     & 0      & 0.0  & 1      & 0.2  & 6      & 1.6  \\
        7     & 0      & 0.0  & 0      & 0.0  & 9      & 2.4  \\
        8     & 0      & 0.0  & 0      & 0.0  & 6      & 1.6  \\
        9     & 0      & 0.0  & 0      & 0.0  & 6      & 1.6  \\
        10    & 0      & 0.0  & 0      & 0.0  & 3      & 0.8  \\
        11+   & 0      & 0.0  & 0      & 0.0  & 50     & 13.2 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \caption{\gls{heapbt} single-guess success probabilities for a random stack snapshot.
    Each stack frame contains three \glspl{heapbt} on average.}
    \label{r2c:tab:heap-ptr-probabilities}
    \begin{tabular}{ll rrr r}
        \toprule
        \textbf{Server} & \textbf{Case} & Depth & Heap Ptr. & \glspl{heapbt} & $P_{Success}$ \\
        \midrule
        \multirow{4}{*}{\propername{nginx}}
        & Worst         & 12    & 40        & 36             & 52.6\%        \\
        & Average       & 22    & 100       & 66             & 60.2\%        \\
        & Median        & 22    & 102       & 66             & 60.7\%        \\
        & Best          & 15    & 74        & 45             & 62.2\%        \\
        \midrule
        \multirow{4}{*}{\propername{Apache}}
        & Worst         & 174   & 194       & 522            & 27.1\%        \\
        & Average       & 140   & 269       & 420            & 39.1\%        \\
        & Median        & 140   & 273       & 420            & 39.4\%        \\
        & Best          & 4     & 29        & 12             & 70.7\%        \\
        \midrule
        \multirow{4}{*}{\propername{Lighttpd}}
        & Worst         & 4     & 4         & 12             & 25.0\%        \\
        & Average       & 5     & 48        & 15             & 71.8\%        \\
        & Median        & 8     & 87        & 24             & 78.4\%        \\
        & Best          & 7     & 116       & 21             & 84.7\%        \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \caption{Function pointer single-guess success probabilities for a random stack frame.
    Each stack frame contains three fake function pointers.}
    \label{r2c:tab:func-ptr-probabilities}
    \begin{tabular}{ll rr r}
        \toprule
        \textbf{Server} & \textbf{Case} & Real Func. Ptr. & Total Func. Ptr. & $P_{Success}$ \\
        \midrule
        \multirow{4}{*}{\propername{nginx}}
        & Worst         & 24              & 27               & 3.7\%         \\
        & Average       & 2.8             & 5.8              & 17.3\%        \\
        & Median        & 1               & 4                & 25.0\%        \\
        & Best          & 1               & 4                & 25.0\%        \\
        \midrule
        \multirow{4}{*}{\propername{Apache}}
        & Worst         & 18              & 21               & 4.8\%         \\
        & Average       & 1.5             & 4.5              & 22.0\%        \\
        & Median        & 1               & 4                & 25.0\%        \\
        & Best          & 1               & 4                & 25.0\%        \\
        \midrule
        \multirow{4}{*}{\propername{Lighttpd}}
        & Worst         & 72              & 75               & 1.3\%         \\
        & Average       & 19.9            & 22.9             & 4.4\%         \\
        & Median        & 2               & 5                & 20.0\%        \\
        & Best          & 1               & 4                & 25.0\%        \\
        \bottomrule
    \end{tabular}
\end{table}

While execute-only memory and function shuffling defeat classic ROP and JIT-ROP attacks, indirect JIT-ROP and \gls{AOCR} remain an issue.
For an indirect JIT-ROP attack, an attacker needs to locate valid code pointers, such as return addresses, in readable memory and infer gadget locations based on the found pointers.
For an \gls{AOCR} attack, an attacker needs to locate function pointers or infer them from other code pointers (\eg return addresses), as well as manipulate function parameters.
In the following subsections we discuss how \rtwoc{} counters each of these attack vectors.
We also discuss the security of stack unwinding tables and the possibility of an attacker corrupting entire or partial code pointers.

Our security evaluation relies on a probabilistic model to estimate an attacker's success rate.
We instantiate the model's variables (such as stack frame composition and pointer distribution) with empirical measurements from \propername{nginx}, \propername{Apache} and \propername{ighttpd}.

\subsubsection{Leaking Return Addresses}\label{r2c:sss:leaking-return-addresses}
\rtwoc protects return addresses with \glspl{BTRA}.
As detailed in \cref{r2c:ss:decoy-mimicry}, the only way for an attacker to identify a return address among the \glspl{BTRA} is by applying brute force.
An attacker's chance to correctly guess the return address depends on the number of \glspl{BTRA} used.
If $B$ is the number of \glspl{BTRA} for a call site, the probability of correctly guessing the return address is given by
\[ P(\mathrm{return address})=\frac{1}{B+1}. \]
Specifically, with 11 \glspl{BTRA} per return address $P(\mathrm{return address})\approx0.09$.
Since return addresses on their own are of limited use, they form only a building block for further inferences.

\subsubsection{Leaking Function Pointers}\label{r2c:sss:leaking-function-pointers}
For an \gls{AOCR}-style whole-function-reuse attack, an attacker needs to locate \emph{specific} function pointers.
That is, the attacker must locate a function pointer with manipulatable parameters and desired functionality.
There are three ways for an attacker to locate function pointers:
\begin{enumerate*}
    \item from the data section;
    \item via the stack, either from function pointers on the stack or indirectly from return addresses
    \item from the heap.
\end{enumerate*}

\rtwoc does not directly protect the heap against information leakage, but aims to prevent an attacker from reaching the heap via the stack via \glspl{heapbt}.
We discuss their respective security properties in \cref{r2c:sss:heap-data}.
\gls{ASLR} randomizes the address of the data section and global-variable shuffling randomizes the order of elements in the data section.
Leaking a function pointer from the data section, therefore, requires a leaked data section pointer first.
Within the data section, an attacker must then locate the desired function pointer.
Exact localization is challenging because of function permutation and boobytrap pointers in the data section.

Leaking a function pointer from the stack can happen either directly or indirectly via a previously leaked return address.
We start with the probability of a direct leak.
Estimating the success probability of locating function pointers on the stack depends on the number of function pointers per stack frame.
Assuming that an attacker can leak arbitrary stack snapshots (see \cref{r2c:s:threat-model}) and can differentiate stack frames within a stack snapshot, stack slot shuffling is currently the only protection.
An attacker can reasonably differentiate function pointers from the return address and \glspl{BTRA} because these occur only at the top of a frame.
With value range analysis an attacker can further differentiate function pointers from other stack slots, such as variables values or heap pointers.
Thus, guessing a function pointer's location means choosing an element from a permutation of all function pointers in the stack frame.
To measure the probability empirically, we have collected stack snapshots from benchmarking workloads on \propername{Apache}, \propername{nginx,} and \propername{lighttp}.
\cref{r2c:tab:func-ptr-dist} shows the function pointer distribution in these snapshots.
About 25\% to 26\% of stack frames (depending on the software) contain no function pointer at all.
In \propername{nginx}, 45\% of all stack frames contain only one function pointer, in \propername{Apache} 35\% and in \propername{lighttp} 14\%.
These cases are troublesome because they mean that an attacker can immediately identify the desired pointer.
Even with two function pointers per frame (between 4.5\% and 30.5\%) the probability of guessing correctly is still 50\%.
We note, however, that only very select function pointers are suitable for \gls{AOCR}'s variant of whole-function-reuse attack and that global variable shuffling further frustrates efforts to locate and manipulate default function parameters.

Leaking a function pointer from a return address requires more guessing.
Let us assume that an attacker has successfully leaked a return address $\mathcal{A}$.
Let us further assume that the attacker \emph{knows} that $\mathcal{A}$ points into a function $\mathcal{F}$.
The attacker can then try to guess the entrypoint of $\mathcal{F}$ for a whole-function-reuse attack.
The function's entrypoint is guaranteed to be before $\mathcal{A}$.
Assuming an average function size of $S$, there are on average $\frac{S}{2}$ addresses before a random return site within $\mathcal{F}$.
With a function alignment of $A$ (typically 16), there are, therefore, on average $\frac{S}{2A}$ possible entry points.
\rtwoc{}'s additional code randomization (see \cref{r2c:ss:strengthening}) decreases the success probability further.
With inserted traps, the function's start address does not coincide with the function's entrypoint.
With $T$ inserted trap instructions, an attacker must guess $\frac{S}{2*A} * T$ possible entrypoints.
The probability of correctly guessing the function's entrypoint thus becomes
\[ P(\mathrm{entrypoint})=\frac{2*A}{S*T}. \]
The average function size $S$ depends on the program, the alignment is typically 16, and the average number of traps depends on a configurable parameter in \rtwoc.
For an alignment $A=16$, a median function size of $S=258$ bytes in a protected \propername{nginx} and an average number of prolog traps $T=3$, this means $P(\mathrm{entrypoint})\approx0.04$.
Estimating based on the median function size is conservative, as the average function size in \propername{nginx} is 789 bytes.
The probability for leaking a single function pointer based on a leaked return address is
\[ P(\mathrm{retaddr\rightarrow{}func}) = P(\mathrm{return address}) * P(\mathrm{entrypoint}). \]
With the concrete numbers from above $P(\mathrm{retaddr\rightarrow{}func})\approx0.0038$.

\subsubsection{Leaking Gadget Addresses}
Instead of mounting a whole-function-reuse attack, an attacker can try to leak enough gadget addresses for a \gls{ROP} attack.
Except for completely blind guessing, an attacker can guess gadget addresses relative to an anchor point.
Such an anchor point can either be a leaked function address (see \cref{r2c:sss:leaking-function-pointers}) or a leaked return address (see \cref{r2c:sss:leaking-return-addresses}).
In either case, the attacker knows the number of call sites between the anchor and the gadget of choice, since \rtwoc does not change the number of call sites.
Each call site has $R_{B}$ \glspl{BTRA} and for each \gls{BTRA}, \rtwoc adds a \code{NOP} with a probability of $P_{NOP}$.
The number of \glspl{BTRA} and the probability of inserted \code{NOP}s are configurable parameters in \rtwoc.
Let $C$ be the average number of call sites within a function in the target program.
We can then estimate the number of call sites between the anchor and a gadget address as $C_{between}=\frac{C}{2}$ on average.
$C_{between}$ is, of course, only a rough estimate that highly depends on $C$.
For \propername{nginx}, we found $C$ to be 4 (median) and 7.3 (average).
Based on these variables, we derive that between the anchor and the gadget, there are $C_{between} * R_{B} * P_{NOP}$ \code{NOP}s on average.
The probability of guessing a gadget address thus becomes
\[ P(\mathrm{gadget})=\frac{1}{C_{between}*B*P_{NOP}}. \]
With 11 \glspl{BTRA} per return address, a probability of $P_{NOP}=0.35$ and $C_{between}=2$, which we used for full \rtwoc, we get $P(\mathrm{gadget})\approx13.9$.
Estimating based on the median number of call sites is conservative, as the average number of call sites per function in \propername{nginx} is 7.
Gadget-based attacks are further impeded by register-allocation randomization.

An attacker typically needs more than one gadget, further decreasing the chance of successfully guessing \emph{all} locations.
For example, the probability of leaking two return addresses and within each function two gadgets is
\[ P\left(\mathrm{2retaddrs\rightarrow{}4gadgets}\right) = P(\mathrm{return address})^2 * P(\mathrm{gadget})^4. \]
With the concrete numbers from above this means $P\left(\mathrm{2retaddrs\rightarrow{}4gadgets}\right)\approx0,00017$.

\subsubsection{Leaking Heap Data}\label{r2c:sss:heap-data}
An attacker can try to leak data from the heap to either learn function pointers or pointers leading to the data section.
As \gls{ASLR} randomizes the heap's location, leaking data requires either a heap over-read vulnerability or a heap pointer.
Such a heap pointer can come either from the data section or the stack.
Unlike with function pointers, the attacker can pick an arbitrary heap pointer to reach the heap.
Heap pointers in the data section are rare, which is why we focus our analysis on the stack.

The probability of correctly guessing a benign heap pointer among all heap pointers depends on the number of benign heap pointers $H$, and the number of \glspl{heapbt} $H_{B}$ in a stack dump.
The number of $H_{B}$ in turn depends on the number of stack frames in the dump because \rtwoc inserts \glspl{heapbt} into each stack frame, depending on a configuration parameter (see \cref{r2c:ss:impl-heap-boobytraps}).
The probability of randomly picking a benign pointer is $\frac{H}{H_{B}+H}$.
The exact number for $H$ is application-specific and depends, for example, on the number of registers containing heap pointers that are spilled to the stack.
\cref{r2c:tab:heap-ptr-probabilities} shows the empirical distribution of heap pointers in stack snapshots taken from \propername{Apache}, \propername{nginx}, and \propername{lighttp} during benchmark workloads.
The table also shows the success probability of randomly guessing a heap pointer among all heap pointers found in a randomly chosen stack snapshot.
As the success probability depends on the concrete composition of the stack snapshot, the table shows the worst, average, and best case for the attacker.
Assuming that an attacker can dump arbitrary many stack snapshots (see \cref{r2c:s:threat-model}), the attacker can repeat the leak until he finds a best-case snapshot.
Generally, snapshots with few stack frames but many heap pointers in those frames constitute the best case for the attacker.
Since \rtwoc inserts a configurable number of \glspl{heapbt} \emph{per frame}, stack snapshots with fewer frames contain also fewer \glspl{heapbt}.
\cref{r2c:tab:heap-ptr-probabilities} also shows, however, that even in the worst case (for the attacker), the success probability still ranges between 25\% and 52.6\%.

The numbers above suggest that \glspl{heapbt} offer only limited protection against heap-based attacks.
We discuss a possible extension of the mimicry principle \emph{onto} the heap in \cref{r2c:ss:heap-mimicry}.

Alternatively to guessing a heap pointer, an attacker could try to identify events where \glspl{heapbt} do not mimic their benign counterparts accurately.
For example, by performing heap feng shui an attacker might be able to identify benign heap pointers with a known distance to each other~\cite{Sotirov2007}.
Note, however, that such an attack requires specific prerequisites and goes significantly beyond the analysis steps of the demonstrated \gls{AOCR} attacks.

Given that \glspl{heapbt} incur overhead but affords only limited protection, an alternative use of the underlying implementation could be to write fake code pointers instead.
\glspl{BTRA} already provide the means for code pointers pointing to booby traps and the \glspl{heapbt} implementation way of writing pointers to random stack slots.
The expected overhead would remain the same or be less because the heap is no longer fragmented by guard pages.
In a function-pointer-guessing scenario, however, the afforded security of pointer mimicry is better.
Recall from \cref{r2c:sss:leaking-function-pointers} that when leaking a function pointer, the attacker needs a \emph{specific} pointer within a \emph{specific} stack frame.
Pointer mimicry could thus help in cases where there are only few function pointers in a stack frame and, as a result, stack slot randomization does not sufficiently protect them (see \cref{r2c:tab:func-ptr-dist}).
Analogous to \cref{r2c:tab:heap-ptr-probabilities}, \cref{r2c:tab:func-ptr-probabilities} contains empirical data together with success probabilities for guessing a function pointer, assuming three fake function pointers per frame on average.
Note that unlike \cref{r2c:tab:heap-ptr-probabilities}, \cref{r2c:tab:func-ptr-probabilities} shows data \emph{per frame}.
Whereas without fake function pointers the best case success probability for the attacker was 100\%, the probability with fake pointers drops to 25\%.
Care must be taken, however, not to write fake code pointers into stack frames without real function pointers.
Since an attacker knows which stack frames contain function pointers in the non-randomized binary, the attacker could identify fake pointers in frames that previously did not contain any function pointers.

\subsubsection{Exception handling and stack unwinding}
\fbetodo{extend with example}
As part of the \gls{BTRA} setup and teardown code, \rtwoc{} also emits the necessary \code{CFI} directives to support exception handling and stack unwinding.
\code{CFI} directives record stack pointer and frame modifications in the \code{.eh\_frame} section.
Since the modifications are recorded relative to the beginning of the frame, decoding the \code{.eh\_frame} section could reveal the location of the return address.
Entries in the \code{.eh\_frame} are not, however, associated with function symbols, but with \gls{PC} \emph{ranges}.
These \gls{PC} ranges are unknown to the attacker due to code layout randomization.
An attacker cannot, therefore, associate entries in the \code{.eh\_frame} table with functions.

The position of an entry in the table---i.e., its row---could provide the attacker with important information.
Each entry in the table, reflects the position of a function in a compilation unit.
% Through function reordering/permutation, the attacker is also deprived of this information.
Through function reordering/permutation row-based references become invalid.
Since exceptions occur infrequently, one could also use a more expensive protection scheme, such as encryption, to protect these meta-data.

\subsubsection{Corrupting code pointers}\label{r2c:sss:corrupting-code-pointers}
\fbetodo{extend section}
\Glspl{CRA} typically corrupt entire code pointers.
An attack called \gls{PIROP} generalizes this principle by corrupting only parts of code pointers and, as a result, is immune to ASLR and page-level randomization~\cite{Goktas2018}.
\rtwoc{} impedes a \gls{PIROP} attack in two ways.
First, \rtwoc{} shuffles functions and randomizes at the sub-function level (see \cref{r2c:ss:strengthening}), thus increasing the entropy for \gls{PIROP}.
Second, \glspl{BTRA} constrain candidate \gls{PIROP} gadgets that manipulate (partial) return addresses:
In the presence of \glspl{BTRA} a \gls{PIROP} attack needs to corrupt \emph{all} return addresses, requiring either iterative gadget execution or more complex gadgets.

\subsection{Remaining attack surface}\label{r2c:ss:remaining-attack-surface}
At present, \rtwoc{} remains susceptible to two types of brute force attacks.
In a Blind ROP scenario with restarting worker processes, an attacker could use \gls{PIROP} to brute force the entropy resulting from \rtwoc{} randomization techniques.
Similarly, an attacker could use the corruption of potential return addresses as a side channel.
For example, by overwriting selected return address candidates with zero and observing whether the process crashes, the attacker could learn the location of the real return address.
Both attacks could be prevented by load time re-randomization.

\rtwoc{} could also deter the corruption of \glspl{BTRA} by checking a random subset of \glspl{BTRA} for consistency after the return.

\rtwoc{} focuses on code-reuse attacks and the leakage of \emph{control-flow} data.
Although \rtwoc{}'s layout randomization raises the bar for attackers~\cite{Hu2016}, \rtwoc{} does not offer the same protection as defenses specialized for data-only attacks~\cite{Carr}.

A way to strengthen \rtwoc{}'s security would be to combine it with \gls{MVEE}s~\cite{Cox2006,Berger2006,Bruschi2007,Volckaert2016,Voulimeneas2020}.
\gls{MVEE}s and diversification defenses like \rtwoc{} naturally complement each other.
Considering that \rtwoc{} diversifies along multiple dimensions, an \gls{MVEE} would detect data corruption or leakage in one of the variants with high probability.
